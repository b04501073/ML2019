{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.ReLU(inplace=True),\n",
    "    \n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            conv_bn(  1,  32, 2), \n",
    "            conv_dw( 32,  64, 1),\n",
    "            conv_dw( 64, 128, 2),\n",
    "            conv_dw(128, 256, 2),\n",
    "            conv_dw(256, 512, 2),\n",
    "            conv_dw(512, 1024, 1),\n",
    "            nn.AvgPool2d(7),\n",
    "        )\n",
    "        self.fc = nn.Linear(1024, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 1, 48, 48)\n"
     ]
    }
   ],
   "source": [
    "f_file = \"data/train.csv\"\n",
    "datas = []\n",
    "with open(f_file) as file:\n",
    "    for line_id, line in enumerate(file):\n",
    "        if line_id == 0:\n",
    "            continue\n",
    "        else:\n",
    "            label, feature = line.split(',')\n",
    "            feature = np.fromstring(feature, dtype=int, sep=' ')\n",
    "            feature = feature.reshape((1, 48, 48))\n",
    "            \n",
    "            datas.append((feature, int(label)))\n",
    "features, labels = zip(*datas)\n",
    "\n",
    "features = np.asarray(features)\n",
    "print(features.shape)\n",
    "# labels = to_categorical(np.asarray(labels, dtype = np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "workers = 4\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datas[2000:],\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=workers, pin_memory=True,\n",
    "    )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datas[:2000],\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    print_freq = 10\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "#         target = target(async=True)\n",
    "        input = input.float()\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1, input.size(0))\n",
    "        top5.update(prec5, input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "#         target = target(async=True)\n",
    "        input = input.float()\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1, input.size(0))\n",
    "        top5.update(prec5, input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/105]\tTime 1.445 (1.445)\tData 0.057 (0.057)\tLoss 1.9815 (1.9815)\tPrec@1 5.078 (5.078)\tPrec@5 71.094 (71.094)\n",
      "Epoch: [0][10/105]\tTime 1.268 (1.298)\tData 0.001 (0.006)\tLoss 2.0631 (1.9014)\tPrec@1 15.234 (21.413)\tPrec@5 87.500 (85.227)\n",
      "Epoch: [0][20/105]\tTime 1.218 (1.277)\tData 0.001 (0.003)\tLoss 1.7994 (1.8797)\tPrec@1 23.828 (22.582)\tPrec@5 89.844 (86.942)\n",
      "Epoch: [0][30/105]\tTime 1.235 (1.262)\tData 0.001 (0.003)\tLoss 1.8094 (1.8607)\tPrec@1 22.266 (23.097)\tPrec@5 90.625 (87.563)\n",
      "Epoch: [0][40/105]\tTime 1.246 (1.252)\tData 0.001 (0.002)\tLoss 1.6942 (1.8379)\tPrec@1 29.688 (24.019)\tPrec@5 91.016 (88.176)\n",
      "Epoch: [0][50/105]\tTime 1.272 (1.249)\tData 0.001 (0.002)\tLoss 1.7363 (1.8249)\tPrec@1 28.516 (24.862)\tPrec@5 91.797 (88.741)\n",
      "Epoch: [0][60/105]\tTime 1.232 (1.248)\tData 0.001 (0.002)\tLoss 1.6458 (1.8099)\tPrec@1 33.594 (25.640)\tPrec@5 95.312 (89.306)\n",
      "Epoch: [0][70/105]\tTime 1.237 (1.248)\tData 0.001 (0.002)\tLoss 1.6504 (1.7862)\tPrec@1 35.156 (27.074)\tPrec@5 93.359 (89.877)\n",
      "Epoch: [0][80/105]\tTime 1.221 (1.245)\tData 0.001 (0.001)\tLoss 1.6930 (1.7689)\tPrec@1 34.375 (28.120)\tPrec@5 92.578 (90.287)\n",
      "Epoch: [0][90/105]\tTime 1.218 (1.243)\tData 0.001 (0.001)\tLoss 1.5521 (1.7532)\tPrec@1 37.500 (28.872)\tPrec@5 91.016 (90.681)\n",
      "Epoch: [0][100/105]\tTime 1.210 (1.242)\tData 0.001 (0.001)\tLoss 1.5751 (1.7402)\tPrec@1 41.016 (29.742)\tPrec@5 93.359 (90.876)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "best_prec1 = 0\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "for epoch in range(200):\n",
    "#     adjust_learning_rate(optimizer, epoch)\n",
    "    lr = lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': args.arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0115,  0.0792, -0.1836,  0.3623, -0.0815,  0.2355,  0.1580],\n",
       "        [-0.0224,  0.0808, -0.1764,  0.2261, -0.1029, -0.0853,  0.0463]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
